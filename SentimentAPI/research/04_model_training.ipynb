{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    vocab_size: int\n",
    "    oov_tok: str\n",
    "    embedding_dim: int\n",
    "    max_length: int # choose based on statistics, for example 150 to 200\n",
    "    padding_type: str\n",
    "    trunc_type: str\n",
    "    units: int #units: The number of hidden units in the layer.\n",
    "    hidden_dense: str\n",
    "    last_dense: str\n",
    "    loss: str\n",
    "    optimizer: str\n",
    "    metrics: str\n",
    "    num_epochs: int\n",
    "    verbose: int\n",
    "    validation_split: int\n",
    "    last_layer: int\n",
    "    dense_layers: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            vocab_size = params.vocab_size,\n",
    "            oov_tok = params.oov_tok,\n",
    "            embedding_dim = params.embedding_dim,\n",
    "            max_length = params.max_length, # choose based on statistics, for example 150 to 200\n",
    "            padding_type =  params.padding_type,\n",
    "            trunc_type = params.trunc_type,\n",
    "            units = params.units, #units: The number of hidden units in the layer.\n",
    "            hidden_dense = params.hidden_dense,\n",
    "            last_dense = params.last_dense,\n",
    "            loss = params.loss,\n",
    "            optimizer = params.optimizer,\n",
    "            metrics = params.metrics,\n",
    "            num_epochs = params.num_epochs,\n",
    "            verbose = params.verbose,\n",
    "            validation_split = params.validation_split,\n",
    "            dense_layers = params.dense_layers,\n",
    "            last_layer = params.last_layer\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from Sentiment import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel('artifacts\\data_transformation\\sentimentDataset\\TransformedData.xlsx')\n",
    "# data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['sentiment'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def loadData(self):\n",
    "        sentiData = pd.read_excel(self.config.data_path)\n",
    "        sentiData.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "        sentiData.dropna(how=\"any\", inplace=True)\n",
    "        return sentiData\n",
    "    \n",
    "    def convertToLower(self,data):\n",
    "        '''\n",
    "        Converting data to the lowercase\n",
    "        '''\n",
    "        print(data.head())\n",
    "        data.loc[:, 'Review'] = data.loc[:, 'Review'].str.lower()\n",
    "        logger.info(f\"Data converted to lowercase\")\n",
    "        return data\n",
    "\n",
    "    def splitData(self,sentiData):\n",
    "        '''\n",
    "        Splitting the data into train and test.\n",
    "        '''\n",
    "        trainSentences, testSentences, trainLabels, testLabels = train_test_split(sentiData['Review'],sentiData['sentiment'] , stratify = sentiData['sentiment'])\n",
    "        return trainSentences, testSentences, trainLabels, testLabels\n",
    "    \n",
    "    def TextToNumeric(self,trainSentences,testSentences):\n",
    "        '''\n",
    "        Converting the text to numeric\n",
    "        '''\n",
    "        tokenizer = Tokenizer(num_words = self.config.vocab_size, oov_token=self.config.oov_tok)\n",
    "        tokenizer.fit_on_texts(trainSentences)\n",
    "        word_index = tokenizer.word_index\n",
    "        # convert train dataset to sequence and pad sequences\n",
    "        trainSequences = tokenizer.texts_to_sequences(trainSentences)\n",
    "        trainPadded = pad_sequences(trainSequences, padding=self.config.padding_type, maxlen=self.config.max_length)\n",
    "        # convert Test dataset to sequence and pad sequences\n",
    "        testSequences = tokenizer.texts_to_sequences(testSentences)\n",
    "        testPadded = pad_sequences(testSequences, padding=self.config.padding_type, maxlen=self.config.max_length)\n",
    "        return trainPadded,testPadded\n",
    "    \n",
    "    def prepareModel(self):\n",
    "        '''\n",
    "        model preparation\n",
    "        '''\n",
    "        # model initialization\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Embedding(self.config.vocab_size, self.config.embedding_dim, input_length=self.config.max_length),\n",
    "            keras.layers.Bidirectional(keras.layers.LSTM(self.config.units)),\n",
    "            keras.layers.Dense(self.config.dense_layers, activation=self.config.hidden_dense),\n",
    "            keras.layers.Dense(self.config.last_layer, activation=self.config.last_dense)\n",
    "        ])\n",
    "        # compile model\n",
    "        model.compile(loss=self.config.loss,\n",
    "                    optimizer=self.config.optimizer,\n",
    "                    metrics=[self.config.metrics])\n",
    "        logger.info(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def trainModel(self,model,trainPadded,trainLabels):\n",
    "            model.fit(trainPadded, trainLabels,epochs=self.config.num_epochs, verbose=self.config.verbose,validation_split=self.config.validation_split)\n",
    "            return model\n",
    "    \n",
    "    def saveModel(self,model) -> bool:\n",
    "         model.save(os.path.join(self.config.root_dir,\"LSTMModel\"+\"V1\"+\".h5\"))\n",
    "         model.save(os.path.join(self.config.model_ckpt,\"LSTMModel\"+\"V1\"+\".h5\"))\n",
    "         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-04 00:48:55,933: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-04 00:48:55,938: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-01-04 00:48:55,939: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-04 00:48:55,944: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-04 00:48:55,946: INFO: common: created directory at: artifacts]\n",
      "[2024-01-04 00:48:55,948: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-01-04 00:48:55,949: INFO: 113507594: data loading Started]\n",
      "[2024-01-04 00:49:13,899: INFO: 113507594: data loading Completed]\n",
      "[2024-01-04 00:49:13,900: INFO: 113507594: data Lowercase convertion Started]\n",
      "                                              Review  sentiment\n",
      "0  Food ok. They good Italian sandwiches. Takes l...          1\n",
      "1  My husband I tried Le Peep twice now. The firs...          1\n",
      "2  This place decent coffee. It's right next movi...          1\n",
      "3  Service great displeased meal. The long line l...          0\n",
      "4  I've heard many good thing place decided give ...          0\n",
      "[2024-01-04 00:49:14,193: INFO: 3502443226: Data converted to lowercase]\n",
      "[2024-01-04 00:49:14,194: INFO: 113507594: data Lowercase convertion Completed]\n",
      "[2024-01-04 00:49:14,195: INFO: 113507594: data Splitting Started]\n",
      "[2024-01-04 00:49:14,308: INFO: 113507594: data Splitting Completed]\n",
      "[2024-01-04 00:49:14,309: INFO: 113507594: data TextToNumeric Started]\n",
      "[2024-01-04 00:49:36,990: INFO: 113507594: data TextToNumeric Completed]\n",
      "[2024-01-04 00:49:36,992: INFO: 113507594: data prepareModel Started]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 100)          300000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[2024-01-04 00:49:37,620: INFO: 3502443226: None]\n",
      "[2024-01-04 00:49:37,622: INFO: 113507594: data prepareModel Completed]\n",
      "[2024-01-04 00:49:37,622: INFO: 113507594: data Model Training Started]\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    logger.info(f\"data loading Started\")\n",
    "    sentiData = model_trainer_config.loadData()\n",
    "    logger.info(f\"data loading Completed\")\n",
    "    logger.info(f\"data Lowercase convertion Started\")\n",
    "    sentiData = model_trainer_config.convertToLower(sentiData)\n",
    "    logger.info(f\"data Lowercase convertion Completed\")\n",
    "    logger.info(f\"data Splitting Started\")\n",
    "    trainSentences, testSentences, trainLabels, testLabels = model_trainer_config.splitData(sentiData)\n",
    "    logger.info(f\"data Splitting Completed\")\n",
    "    logger.info(f\"data TextToNumeric Started\")\n",
    "    trainPadded,testPadded = model_trainer_config.TextToNumeric(trainSentences,testSentences)\n",
    "    logger.info(f\"data TextToNumeric Completed\")\n",
    "    logger.info(f\"data prepareModel Started\")\n",
    "    modelArchitectute = model_trainer_config.prepareModel()\n",
    "    logger.info(f\"data prepareModel Completed\")\n",
    "    logger.info(f\"data Model Training Started\")\n",
    "    trainedModel = model_trainer_config.trainModel(modelArchitectute,trainPadded,trainLabels)\n",
    "    logger.info(f\"data Model Training Completed\")\n",
    "    model_trainer_config.saveModel(trainedModel)\n",
    "    logger.info(f\"model saved successfully\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokens(plaintext):\n",
    "        text = []\n",
    "        # print(plaintext)\n",
    "        text.append(plaintext)\n",
    "        # print(text)\n",
    "        vocab_size = 3000 # choose based on statistics\n",
    "        oov_tok = ''\n",
    "        max_length = 200\n",
    "        # text = [\"This is good i have ever ate\"]\n",
    "        tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        sequences = tokenizer.texts_to_sequences(text)\n",
    "        # print(\"sequences\",sequences)\n",
    "        # pad the sequence\n",
    "        padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "        # print(padded)\n",
    "        return padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
