{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    METRIC_FILE: Path\n",
    "    mlflow_uri: str\n",
    "    all_params: dict\n",
    "    model: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories\n",
    "from Sentiment.entity.config_entity import DataIngestionConfig, DataValidationConfig, DataTransformationConfig, ModelTrainerConfig, ModelEvaluationConfig\n",
    "from Sentiment import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_present_dir,\n",
    "            data_path=config.data_present_path,\n",
    "            save_dir=config.root_dir,\n",
    "            save_path=config.data_path\n",
    "        )\n",
    "        return data_transformation_config\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            vocab_size = params.vocab_size,\n",
    "            oov_tok = params.oov_tok,\n",
    "            embedding_dim = params.embedding_dim,\n",
    "            max_length = params.max_length, # choose based on statistics, for example 150 to 200\n",
    "            padding_type =  params.padding_type,\n",
    "            trunc_type = params.trunc_type,\n",
    "            units = params.units, #units: The number of hidden units in the layer.\n",
    "            hidden_dense = params.hidden_dense,\n",
    "            last_dense = params.last_dense,\n",
    "            loss = params.loss,\n",
    "            optimizer = params.optimizer,\n",
    "            metrics = params.metrics,\n",
    "            num_epochs = params.num_epochs,\n",
    "            verbose = params.verbose,\n",
    "            validation_split = params.validation_split,\n",
    "            dense_layers = params.dense_layers,\n",
    "            last_layer = params.last_layer\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            METRIC_FILE = config.METRIC_FILE,\n",
    "            mlflow_uri=\"https://dagshub.com/AKISHPOTHURI/sentimentanalysis.mlflow\",\n",
    "            all_params=self.params,\n",
    "            model = config.model\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Sentiment import logger\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tokenizers import Tokenizer\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def loadModel(self):\n",
    "        model = keras.models.load_model(self.config.model)\n",
    "        return model\n",
    "    \n",
    "    def predictTest(self,testSentences,model):\n",
    "        \n",
    "        return model.predict(testSentences)\n",
    "    \n",
    "    def sentimentClassify(self,predictions):\n",
    "        prediction = []\n",
    "        for i in predictions:\n",
    "            if i > 50:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "        return prediction\n",
    "    \n",
    "    def TextToNumeric(self,testSentences):\n",
    "        '''\n",
    "        Converting the text to numeric\n",
    "        '''\n",
    "        try:\n",
    "            vocab_size = 3000 # choose based on statistics\n",
    "            oov_tok = ''\n",
    "            max_length = 200\n",
    "            padding_type = 'post'\n",
    "            tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(testSentences)\n",
    "            word_index = tokenizer.word_index\n",
    "            # convert Test dataset to sequence and pad sequences\n",
    "            testSequences = tokenizer.texts_to_sequences(testSentences)\n",
    "            testPadded = pad_sequences(testSequences, padding=padding_type, maxlen=max_length)\n",
    "            return testPadded\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "            \n",
    "    def log_into_mlflow(self,accuracy,model):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(\n",
    "                {\"accuracy\": accuracy}\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.keras.log_model(model, \"model\",registered_model_name=\"LSTMModel\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(model,\"model\")\n",
    "    \n",
    "    def EvaluationMetrics(self,predictions,testLabels):\n",
    "        try:\n",
    "            accuracyScore = accuracy_score(testLabels,predictions)\n",
    "            return accuracyScore\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def ModelEvaluationStatus(self,accuracyScore):\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            all_files = os.listdir(os.path.join(\"artifacts\",\"model_evaluation\"))\n",
    "\n",
    "            for file in all_files:\n",
    "                if file not in self.config.ALL_REQUIRED_FILES:\n",
    "                    Evaluation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\")\n",
    "                else:\n",
    "                    Evaluation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\"+\",\")\n",
    "                        f.write(f\"accuracyScore: {accuracyScore}\"+\",\")\n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.components.model_trainer import ModelTrainer\n",
    "from Sentiment.components.model_evaluation import ModelEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/AKISHPOTHURI/sentimentanalysis.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"AKISHPOTHURI\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"cf4ffa918ff3684246b7d2174e3038bddff682c3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-19 12:12:19,459: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-19 12:12:19,468: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-01-19 12:12:19,469: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-19 12:12:19,474: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-19 12:12:19,476: INFO: common: created directory at: artifacts]\n",
      "[2024-01-19 12:12:19,478: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-01-19 12:12:19,479: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2024-01-19 12:12:19,481: INFO: 785128617: data loading Started]\n",
      "[2024-01-19 12:12:46,787: INFO: 785128617: data loading Completed]\n",
      "[2024-01-19 12:12:46,789: INFO: 785128617: data Lowercase convertion Started]\n",
      "[2024-01-19 12:12:47,067: INFO: model_trainer: Data converted to lowercase]\n",
      "[2024-01-19 12:12:47,069: INFO: 785128617: data Lowercase convertion Completed]\n",
      "[2024-01-19 12:12:47,071: INFO: 785128617: data Splitting Started]\n",
      "[2024-01-19 12:12:47,275: INFO: 785128617: data Splitting Completed]\n",
      "[2024-01-19 12:12:47,277: INFO: 785128617: tokenizing the Text Started]\n",
      "[2024-01-19 12:12:55,609: INFO: 785128617: tokenizing the Text Completed]\n",
      "[2024-01-19 12:12:55,610: INFO: 785128617: Model Loading]\n",
      "[2024-01-19 12:12:56,531: INFO: 785128617: Model Loaded]\n",
      "[2024-01-19 12:12:56,532: INFO: 785128617: Predicting Test Data]\n",
      "2247/2247 [==============================] - 108s 47ms/step\n",
      "[2024-01-19 12:14:45,645: INFO: 785128617: Predicting Test Data Completed]\n",
      "[2024-01-19 12:14:45,646: INFO: 785128617: Predicting classification Started]\n",
      "[2024-01-19 12:14:45,804: INFO: 785128617: Predicting classification Completed]\n",
      "[2024-01-19 12:14:45,805: INFO: 785128617: EvaluationMetrics Started]\n",
      "[2024-01-19 12:14:45,837: INFO: 785128617: EvaluationMetrics Completed]\n",
      "[2024-01-19 12:14:45,838: INFO: 785128617: Updating MLFLOW started]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/01/19 12:14:48 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-19 12:14:58,978: WARNING: save: Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.]\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\tmpt_k5abmb\\model\\data\\model\\assets\n",
      "[2024-01-19 12:15:00,692: INFO: builder_impl: Assets written to: C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\tmpt_k5abmb\\model\\data\\model\\assets]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'LSTMModel'.\n",
      "2024/01/19 12:15:37 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: LSTMModel, version 1\n",
      "Created version '1' of model 'LSTMModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-19 12:15:37,559: INFO: 785128617: Updating MLFLOW Completed]\n",
      "True\n",
      "[2024-01-19 12:15:37,580: INFO: 785128617: EvaluationMetrics Saved]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    logger.info(f\"data loading Started\")\n",
    "    sentiData = model_trainer_config.loadData()\n",
    "    logger.info(f\"data loading Completed\")\n",
    "    logger.info(f\"data Lowercase convertion Started\")\n",
    "    sentiData = model_trainer_config.convertToLower(sentiData)\n",
    "    logger.info(f\"data Lowercase convertion Completed\")\n",
    "    logger.info(f\"data Splitting Started\")\n",
    "    trainSentences, testSentences, trainLabels, testLabels = model_trainer_config.splitData(sentiData)\n",
    "    logger.info(f\"data Splitting Completed\")\n",
    "    logger.info(f\"tokenizing the Text Started\")\n",
    "    testPadded = model_evaluation_config.TextToNumeric(testSentences)\n",
    "    logger.info(f\"tokenizing the Text Completed\")\n",
    "    logger.info(f\"Model Loading\")\n",
    "    model = model_evaluation_config.loadModel()\n",
    "    logger.info(f\"Model Loaded\")\n",
    "    logger.info(f\"Predicting Test Data\")\n",
    "    predictions = model_evaluation_config.predictTest(testPadded,model)\n",
    "    logger.info(f\"Predicting Test Data Completed\")\n",
    "    logger.info(f\"Predicting classification Started\")\n",
    "    prediction = model_evaluation_config.sentimentClassify(predictions)\n",
    "    logger.info(f\"Predicting classification Completed\")\n",
    "    logger.info(f\"EvaluationMetrics Started\")\n",
    "    accuracyScore = model_evaluation_config.EvaluationMetrics(prediction,testLabels)\n",
    "    logger.info(f\"EvaluationMetrics Completed\")\n",
    "    logger.info(f\"Updating MLFLOW started\")\n",
    "    model_evaluation_config.log_into_mlflow(accuracyScore,model)\n",
    "    logger.info(f\"Updating MLFLOW Completed\")\n",
    "    status = model_evaluation_config.ModelEvaluationStatus(accuracyScore)\n",
    "    print(status)\n",
    "    logger.info(f\"EvaluationMetrics Saved\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
