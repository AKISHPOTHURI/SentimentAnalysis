{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    METRIC_FILE: Path\n",
    "    model: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories\n",
    "from Sentiment.entity.config_entity import DataIngestionConfig, DataValidationConfig, DataTransformationConfig, ModelTrainerConfig, ModelEvaluationConfig\n",
    "from Sentiment import logger\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir \n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "    \n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n",
    "        )\n",
    "\n",
    "        return data_validation_config\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_present_dir,\n",
    "            data_path=config.data_present_path,\n",
    "            save_dir=config.root_dir,\n",
    "            save_path=config.data_path\n",
    "        )\n",
    "        return data_transformation_config\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            data_path = config.data_path,\n",
    "            model_ckpt = config.model_ckpt,\n",
    "            vocab_size = params.vocab_size,\n",
    "            oov_tok = params.oov_tok,\n",
    "            embedding_dim = params.embedding_dim,\n",
    "            max_length = params.max_length, # choose based on statistics, for example 150 to 200\n",
    "            padding_type =  params.padding_type,\n",
    "            trunc_type = params.trunc_type,\n",
    "            units = params.units, #units: The number of hidden units in the layer.\n",
    "            hidden_dense = params.hidden_dense,\n",
    "            last_dense = params.last_dense,\n",
    "            loss = params.loss,\n",
    "            optimizer = params.optimizer,\n",
    "            metrics = params.metrics,\n",
    "            num_epochs = params.num_epochs,\n",
    "            verbose = params.verbose,\n",
    "            validation_split = params.validation_split,\n",
    "            dense_layers = params.dense_layers,\n",
    "            last_layer = params.last_layer\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            METRIC_FILE = config.METRIC_FILE,\n",
    "            model = config.model\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Sentiment import logger\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def loadModel(self):\n",
    "        model = keras.models.load_model(self.config.model)\n",
    "        return model\n",
    "    \n",
    "    def predictTest(self,testSentences,model):\n",
    "        \n",
    "        return model.predict(testSentences)\n",
    "    \n",
    "    def sentimentClassify(self,predictions):\n",
    "        prediction = []\n",
    "        for i in predictions:\n",
    "            if i > 50:\n",
    "                prediction.append(1)\n",
    "            else:\n",
    "                prediction.append(0)\n",
    "        return prediction\n",
    "    \n",
    "    def TextToNumeric(self,testSentences):\n",
    "        '''\n",
    "        Converting the text to numeric\n",
    "        '''\n",
    "        try:\n",
    "            vocab_size = 3000 # choose based on statistics\n",
    "            oov_tok = ''\n",
    "            max_length = 200\n",
    "            padding_type = 'post'\n",
    "            tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(testSentences)\n",
    "            word_index = tokenizer.word_index\n",
    "            # convert Test dataset to sequence and pad sequences\n",
    "            testSequences = tokenizer.texts_to_sequences(testSentences)\n",
    "            testPadded = pad_sequences(testSequences, padding=padding_type, maxlen=max_length)\n",
    "            return testPadded\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    \n",
    "    def EvaluationMetrics(self,predictions,testLabels):\n",
    "        try:\n",
    "            accuracyScore = accuracy_score(testLabels,predictions)\n",
    "            recallScore = recall_score(testLabels,predictions)\n",
    "            precisionScore = precision_score(testLabels,predictions)\n",
    "            f1Score = f1_score(testLabels,predictions)\n",
    "            return accuracyScore,recallScore,precisionScore,f1Score\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def ModelEvaluationStatus(self,accuracyScore,recallScore,precisionScore,f1Score):\n",
    "        try:\n",
    "            validation_status = None\n",
    "\n",
    "            all_files = os.listdir(os.path.join(\"artifacts\",\"model_evaluation\"))\n",
    "\n",
    "            for file in all_files:\n",
    "                if file not in self.config.ALL_REQUIRED_FILES:\n",
    "                    Evaluation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\")\n",
    "                else:\n",
    "                    Evaluation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\"+\",\")\n",
    "                        f.write(f\"accuracyScore: {accuracyScore}\"+\",\")\n",
    "                        f.write(f\"recallScore: {recallScore}\"+\",\")\n",
    "                        f.write(f\"precisionScore: {precisionScore}\"+\",\")\n",
    "                        f.write(f\"f1Score: {f1Score}\"+\",\")\n",
    "\n",
    "            return validation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.components.model_trainer import ModelTrainer\n",
    "from Sentiment.components.model_evaluation import ModelEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.config.configuration import ConfigurationManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-06 02:14:00,600: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-06 02:14:00,603: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-01-06 02:14:00,605: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-06 02:14:00,606: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-06 02:14:00,607: INFO: common: created directory at: artifacts]\n",
      "[2024-01-06 02:14:00,608: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-01-06 02:14:00,609: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2024-01-06 02:14:00,610: INFO: 3575743728: data loading Started]\n",
      "[2024-01-06 02:14:13,578: INFO: 3575743728: data loading Completed]\n",
      "[2024-01-06 02:14:13,579: INFO: 3575743728: data Lowercase convertion Started]\n",
      "[2024-01-06 02:14:13,832: INFO: model_trainer: Data converted to lowercase]\n",
      "[2024-01-06 02:14:13,833: INFO: 3575743728: data Lowercase convertion Completed]\n",
      "[2024-01-06 02:14:13,834: INFO: 3575743728: data Splitting Started]\n",
      "[2024-01-06 02:14:13,983: INFO: 3575743728: data Splitting Completed]\n",
      "[2024-01-06 02:14:13,984: INFO: 3575743728: tokenizing the Text Started]\n",
      "[2024-01-06 02:14:18,360: INFO: 3575743728: tokenizing the Text Completed]\n",
      "[2024-01-06 02:14:18,364: INFO: 3575743728: Model Loading]\n",
      "[2024-01-06 02:14:18,695: INFO: 3575743728: Model Loaded]\n",
      "[2024-01-06 02:14:18,695: INFO: 3575743728: Predicting Test Data]\n",
      "2247/2247 [==============================] - 56s 25ms/step\n",
      "[2024-01-06 02:15:15,270: INFO: 3575743728: Predicting Test Data Completed]\n",
      "[2024-01-06 02:15:15,272: INFO: 3575743728: Predicting classification Started]\n",
      "[2024-01-06 02:15:15,359: INFO: 3575743728: Predicting classification Completed]\n",
      "[2024-01-06 02:15:15,360: INFO: 3575743728: EvaluationMetrics Started]\n",
      "[2024-01-06 02:15:15,476: INFO: 3575743728: EvaluationMetrics Completed]\n",
      "None\n",
      "[2024-01-06 02:15:15,477: INFO: 3575743728: EvaluationMetrics Saved]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akish\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "    logger.info(f\"data loading Started\")\n",
    "    sentiData = model_trainer_config.loadData()\n",
    "    logger.info(f\"data loading Completed\")\n",
    "    logger.info(f\"data Lowercase convertion Started\")\n",
    "    sentiData = model_trainer_config.convertToLower(sentiData)\n",
    "    logger.info(f\"data Lowercase convertion Completed\")\n",
    "    logger.info(f\"data Splitting Started\")\n",
    "    trainSentences, testSentences, trainLabels, testLabels = model_trainer_config.splitData(sentiData)\n",
    "    logger.info(f\"data Splitting Completed\")\n",
    "    logger.info(f\"tokenizing the Text Started\")\n",
    "    testPadded = model_evaluation_config.TextToNumeric(testSentences)\n",
    "    logger.info(f\"tokenizing the Text Completed\")\n",
    "    logger.info(f\"Model Loading\")\n",
    "    model = model_evaluation_config.loadModel()\n",
    "    logger.info(f\"Model Loaded\")\n",
    "    logger.info(f\"Predicting Test Data\")\n",
    "    predictions = model_evaluation_config.predictTest(testPadded,model)\n",
    "    logger.info(f\"Predicting Test Data Completed\")\n",
    "    logger.info(f\"Predicting classification Started\")\n",
    "    prediction = model_evaluation_config.sentimentClassify(predictions)\n",
    "    logger.info(f\"Predicting classification Completed\")\n",
    "    logger.info(f\"EvaluationMetrics Started\")\n",
    "    accuracyScore,recallScore,precisionScore,f1Score = model_evaluation_config.EvaluationMetrics(prediction,testLabels)\n",
    "    logger.info(f\"EvaluationMetrics Completed\")\n",
    "    status = model_evaluation_config.ModelEvaluationStatus(accuracyScore,recallScore,precisionScore,f1Score)\n",
    "    print(status)\n",
    "    logger.info(f\"EvaluationMetrics Saved\")\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "recallScore = metrics.f1_score(testLabels,prediction,average='weighted',zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.324263889291608"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recallScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
