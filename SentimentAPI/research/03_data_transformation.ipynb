{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    save_dir: Path\n",
    "    save_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_present_dir,\n",
    "            data_path=config.data_present_path,\n",
    "            save_dir=config.root_dir,\n",
    "            save_path=config.data_path\n",
    "            # tokenizer_name = config.tokenizer_name\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akish\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-01 23:39:59,392: INFO: config: TensorFlow version 2.12.0 available.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def removePuntuations(self,text) -> str:\n",
    "        punctuation = string.punctuation\n",
    "        return text.translate(str.maketrans('', '', punctuation))\n",
    "\n",
    "    def removeStopwords(self,text) -> str:\n",
    "        newText = []\n",
    "\n",
    "        for word in text.split():\n",
    "            if word in stopwords.words('english'):\n",
    "                newText.append('')\n",
    "            else:\n",
    "                newText.append(word)\n",
    "        x = newText[:]\n",
    "        newText.clear()\n",
    "        return \" \".join(x)\n",
    "\n",
    "\n",
    "    def lemmatizeText(self,text) -> str:\n",
    "        w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        sentence = \"\"\n",
    "        for word in w_tokenizer.tokenize(text):\n",
    "            sentence = sentence + lemmatizer.lemmatize(word) + \" \"\n",
    "        return sentence\n",
    "\n",
    "    def chatConversion(self,text) -> str:\n",
    "        newText = []\n",
    "        for word in text.split():\n",
    "            if word.upper() in chatWords:\n",
    "                newText.append(chatWords[word.upper()])\n",
    "            else:\n",
    "                newText.append(word)\n",
    "        return \" \".join(newText)\n",
    "\n",
    "    def decodeEmoji(self,text) -> str:\n",
    "        return emoji.demojize(text).replace(\":\",'').replace(\"_\",\" \")\n",
    "\n",
    "    def correctText(self,text) -> str:\n",
    "        return TextBlob(text).correct().string\n",
    "\n",
    "    def labelling(self,sentiment):\n",
    "        if sentiment == 'positive':\n",
    "            return 1\n",
    "        elif sentiment == 'negative':\n",
    "            return 0\n",
    "        elif sentiment == 1:\n",
    "            return 1\n",
    "        elif sentiment == 0:\n",
    "            return 0\n",
    "\n",
    "    def shuffle(self,sentiData):\n",
    "        for i in range(50):\n",
    "            sentiData = sklearn.utils.shuffle(sentiData)\n",
    "        return sentiData\n",
    "\n",
    "    def load(self):\n",
    "        sentiData = pd.read_excel(self.config.data_path)\n",
    "        return sentiData\n",
    "    \n",
    "    def saveToExcel(self,sentiData):\n",
    "            return sentiData.to_excel(os.path.join(self.config.save_path))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-01 23:46:10,993: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-01 23:46:10,995: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-01-01 23:46:10,997: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-01 23:46:10,998: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-01 23:46:10,999: INFO: common: created directory at: artifacts]\n",
      "[2024-01-01 23:46:11,000: INFO: common: created directory at: artifacts/data_transformation/sentimentDataset]\n",
      "artifacts/data_ingestion/SentimentDataFinal.xlsx\n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                              Review sentiment  \n",
      "0  If you decide to eat here, just be aware it is...  positive  \n",
      "1  I've taken a lot of spin classes over the year...  positive  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...  positive  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...  positive  \n",
      "4  Cute interior and owner (?) gave us tour of up...  positive  \n",
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                              Review sentiment  \n",
      "0  If you decide to eat here, just be aware it is...  positive  \n",
      "1  I've taken a lot of spin classes over the year...  positive  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...  positive  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...  positive  \n",
      "4  Cute interior and owner (?) gave us tour of up...  positive  \n",
      "artifacts/data_transformation/sentimentDataset/TransformedData.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    print(data_transformation.config.data_path)\n",
    "    sentiData = data_transformation.load()\n",
    "    print(sentiData.head())\n",
    "    data_transformation.saveToExcel(sentiData)\n",
    "    # sentiData = data_transformation.shuffle(data)\n",
    "    # print(sentiData.head())\n",
    "    # sentiData['sentiment'] = sentiData['sentiment'].apply(data_transformation.labelling)\n",
    "    # print(sentiData.head())\n",
    "    # # sentiData['Review'] = sentiData['Review'].apply(data_transformation.removePuntuations)\n",
    "    # # print(\"------------------handling punctuations done---------------------\")\n",
    "    # print(sentiData.head())\n",
    "    # sentiData['Review'] = sentiData['Review'].apply(data_transformation.removeStopwords)\n",
    "    # print(\"---------------------removing stopwords done---------------------\")\n",
    "    # print(sentiData.head())\n",
    "    # sentiData['Review'] = sentiData['Review'].apply(data_transformation.lemmatizeText)\n",
    "    # print(\"--------------------lemmatizing done-----------------------\")\n",
    "    # print(sentiData.head())\n",
    "    # data_transformation.saveToExcel(sentiData)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "def load(self):\n",
    "    b = load_dataset(self.config.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentimentData['Review'], encoded_labels, stratify = encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200 # choose based on statistics, for example 150 to 200\n",
    "# padding_type='post'\n",
    "trunc_type='post'\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
