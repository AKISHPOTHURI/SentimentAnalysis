{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import ConfigBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"key\":\"value\",\"key1\":\"value1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ConfigBox(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'value'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downlaod Data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://drive.google.com/file/d/1gvXLFxxoqawVubViRigjSK0m8VPsJHWM/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1gvXLFxxoqawVubViRigjSK0m8VPsJHWM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileId = url.split(\"/\")[-2]\n",
    "fileId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?/export=download&id=1gvXLFxxoqawVubViRigjSK0m8VPsJHWM\n",
      "From (redirected): https://drive.google.com/uc?/export=download&id=1gvXLFxxoqawVubViRigjSK0m8VPsJHWM&confirm=t&uuid=105f37c5-b9e6-483c-b0aa-390a3643b5ba\n",
      "To: c:\\Users\\akish\\SentimentAnalysis\\SentimentAPI\\research\\SentimentDataFinal.zip\n",
      "100%|██████████| 74.5M/74.5M [00:27<00:00, 2.67MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SentimentDataFinal.zip'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'https://drive.google.com/uc?/export=download&id='\n",
    "gdown.download(prefix+fileId, \"SentimentDataFinal.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.constants import *\n",
    "from Sentiment.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('artifacts\\data_transformation\\sentimentDataset\\TransformedFinalData.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.6.0 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (4.6.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (2.31.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (0.1.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from transformers==4.6.0) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from tqdm>=4.27->transformers==4.6.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from requests->transformers==4.6.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from requests->transformers==4.6.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from requests->transformers==4.6.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from requests->transformers==4.6.0) (2023.11.17)\n",
      "Requirement already satisfied: click in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from sacremoses->transformers==4.6.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from sacremoses->transformers==4.6.0) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "data.dropna(how=\"any\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review=list(data['Review'])\n",
    "sentiment=list(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Review, sentiment, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 13.2MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.13MB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 55.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(X_test, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),y_train\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),y_test\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./results/logs',\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 15:31:18,943: WARNING: control_flow: The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).]\n",
      "[2024-01-05 15:31:19,062: WARNING: control_flow: The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 697, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 639, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 622, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 744, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in tf__call\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in <lambda>\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification_1' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 816, in call  *\n            loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss  **\n            return self.compiled_loss(\n    \n        TypeError: 'NoneType' object is not callable\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification_1' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(8, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(8, 512), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(8,), dtype=int32)\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m TFDistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TFTrainer(\n\u001b[0;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[0;32m      6\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m      7\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m      8\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py:555\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m     steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_training_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m=\u001b[39m iterations\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_logging \u001b[38;5;241m=\u001b[39m epoch_iter \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyakgzytv.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__distributed_training_steps\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m nb_instances_in_batch \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_compute_nb_instances, (ag__\u001b[38;5;241m.\u001b[39mld(batch),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_get_step_inputs, (ag__\u001b[38;5;241m.\u001b[39mld(batch), ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_batch)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrun, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mapply_gradients, ag__\u001b[38;5;241m.\u001b[39mld(inputs)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filerxs_sw1r.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m _ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    110\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradients\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt((ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m), if_body_4, else_body_4, get_state_5, set_state_5, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filerxs_sw1r.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients.<locals>.if_body_4\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_4\u001b[39m():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m features, labels\n\u001b[1;32m---> 18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_instances_in_global_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileimk28a1f.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m (per_example_loss, _) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun_model, (ag__\u001b[38;5;241m.\u001b[39mld(features), ag__\u001b[38;5;241m.\u001b[39mld(labels), \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m scaled_loss \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(per_example_loss) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_global_batch),), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(per_example_loss)\u001b[38;5;241m.\u001b[39mdtype), fscope))\n\u001b[0;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgradients, (ag__\u001b[38;5;241m.\u001b[39mld(scaled_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyq1keb2l.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model\u001b[1;34m(self, features, labels, training)\u001b[0m\n\u001b[0;32m     38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel, (ag__\u001b[38;5;241m.\u001b[39mld(features),), \u001b[38;5;28mdict\u001b[39m(labels\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(labels), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m (loss, logits) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyq1keb2l.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model.<locals>.else_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_1\u001b[39m():\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m outputs\n\u001b[1;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]), fscope)\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp((ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute_loss, (ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(logits)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(inputs[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] is None)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]), fscope)\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp((ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(inputs[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] is None)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 697, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 639, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 622, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 744, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in tf__call\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in <lambda>\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification_1' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 816, in call  *\n            loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss  **\n            return self.compiled_loss(\n    \n        TypeError: 'NoneType' object is not callable\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification_1' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(8, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(8, 512), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(8,), dtype=int32)\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 15:29:53,074: WARNING: control_flow: The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).]\n",
      "[2024-01-05 15:29:54,715: WARNING: ag_logging: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1fe87f52400>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert]\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x1fe87f52400>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[2024-01-05 15:29:55,283: WARNING: control_flow: The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 697, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 639, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 622, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 744, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in tf__call\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in <lambda>\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 816, in call  *\n            loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss  **\n            return self.compiled_loss(\n    \n        TypeError: 'NoneType' object is not callable\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(8, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(8, 512), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(8,), dtype=int32)\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py:555\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m     steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 555\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_training_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m=\u001b[39m iterations\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_logging \u001b[38;5;241m=\u001b[39m epoch_iter \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyakgzytv.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__distributed_training_steps\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m nb_instances_in_batch \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_compute_nb_instances, (ag__\u001b[38;5;241m.\u001b[39mld(batch),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_get_step_inputs, (ag__\u001b[38;5;241m.\u001b[39mld(batch), ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_batch)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mrun, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mapply_gradients, ag__\u001b[38;5;241m.\u001b[39mld(inputs)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filerxs_sw1r.py:111\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m    109\u001b[0m _ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    110\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradients\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt((ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m), if_body_4, else_body_4, get_state_5, set_state_5, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filerxs_sw1r.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply_gradients.<locals>.if_body_4\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body_4\u001b[39m():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m features, labels\n\u001b[1;32m---> 18\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_instances_in_global_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mlist\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gradients), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileimk28a1f.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__training_step\u001b[1;34m(self, features, labels, nb_instances_in_global_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 11\u001b[0m (per_example_loss, _) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun_model, (ag__\u001b[38;5;241m.\u001b[39mld(features), ag__\u001b[38;5;241m.\u001b[39mld(labels), \u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m scaled_loss \u001b[38;5;241m=\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(per_example_loss) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (ag__\u001b[38;5;241m.\u001b[39mld(nb_instances_in_global_batch),), \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(per_example_loss)\u001b[38;5;241m.\u001b[39mdtype), fscope))\n\u001b[0;32m     13\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mgradients, (ag__\u001b[38;5;241m.\u001b[39mld(scaled_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyq1keb2l.py:40\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model\u001b[1;34m(self, features, labels, training)\u001b[0m\n\u001b[0;32m     38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mmodel, (ag__\u001b[38;5;241m.\u001b[39mld(features),), \u001b[38;5;28mdict\u001b[39m(labels\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(labels), training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(labels), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_1, else_body_1, get_state_1, set_state_1, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m (loss, logits) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(outputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_2\u001b[39m():\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_fileyq1keb2l.py:38\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_model.<locals>.else_body_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_1\u001b[39m():\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m outputs\n\u001b[1;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]), fscope)\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp((ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcompute_loss, (ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], ag__\u001b[38;5;241m.\u001b[39mld(logits)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(inputs[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] is None)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "File \u001b[1;32mC:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py:18\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdropout, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]), fscope)\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mclassifier, (ag__\u001b[38;5;241m.\u001b[39mld(pooled_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mif_exp((ag__\u001b[38;5;241m.\u001b[39mld(inputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mNone\u001b[39;00m), (\u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(inputs[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m] is None)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, retval_)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 697, in distributed_training_steps  *\n        self.args.strategy.run(self.apply_gradients, inputs)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 639, in apply_gradients  *\n        gradients = self.training_step(features, labels, nb_instances_in_global_batch)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 622, in training_step  *\n        per_example_loss, _ = self.run_model(features, labels, True)\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\trainer_tf.py\", line 744, in run_model  *\n        outputs = self.model(features, labels=labels, training=training)[:2]\n    File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in tf__call\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n    File \"C:\\Users\\AKISH~1.POT\\AppData\\Local\\Temp\\__autograph_generated_filekd9yn5el.py\", line 18, in <lambda>\n        loss = ag__.if_exp((ag__.ld(inputs)['labels'] is None), (lambda : None), (lambda : ag__.converted_call(ag__.ld(self).compute_loss, (ag__.ld(inputs)['labels'], ag__.ld(logits)), None, fscope)), \"(inputs['labels'] is None)\")\n\n    TypeError: Exception encountered when calling layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py\", line 816, in call  *\n            loss = None if inputs[\"labels\"] is None else self.compute_loss(inputs[\"labels\"], logits)\n        File \"c:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss  **\n            return self.compiled_loss(\n    \n        TypeError: 'NoneType' object is not callable\n    \n    \n    Call arguments received by layer 'tf_distil_bert_for_sequence_classification' (type TFDistilBertForSequenceClassification):\n      • input_ids={'input_ids': 'tf.Tensor(shape=(8, 512), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(8, 512), dtype=int32)'}\n      • attention_mask=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=tf.Tensor(shape=(8,), dtype=int32)\n      • training=True\n      • kwargs=<class 'inspect._empty'>\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\akish.pothuri\\\\python\\\\SentimentAnalysis\\\\SentimentAPI'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Sentiment import logger\n",
    "import keras\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "from Sentiment.entity.config_entity import ModelEvaluationConfig\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def loadModel(self):\n",
    "        '''\n",
    "        Model Loading from the artifacts\n",
    "        '''\n",
    "        model = keras.models.load_model(os.path.join(self.config.model,\"LSTMModel\"+\"V1\"+\".h5\"))\n",
    "        return model\n",
    "    \n",
    "    def predictTest(self,testSentences,model):\n",
    "        '''\n",
    "        test Sentence prediction\n",
    "        '''\n",
    "        predictions = model.predict(testSentences)\n",
    "        return predictions\n",
    "    \n",
    "    def EvaluationMetrics(self,predictions,testLabels):\n",
    "        '''\n",
    "        Finding evaluation metrics\n",
    "        '''\n",
    "        try:\n",
    "            accuracyScore = accuracy_score(testLabels,predictions)\n",
    "            recallScore = recall_score(testLabels,predictions)\n",
    "            precisionScore = precision_score(testLabels,predictions)\n",
    "            f1Score = f1_score(testLabels,predictions)\n",
    "            return accuracyScore,recallScore,precisionScore,f1Score\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def ModelEvaluationStatus(self,accuracyScore,recallScore,precisionScore,f1Score) -> bool:\n",
    "        '''\n",
    "        storing the metrics results in a text file\n",
    "        '''\n",
    "        try:\n",
    "            Evaluation_status = None\n",
    "\n",
    "            all_files = os.listdir(os.path.join(\"artifacts\",\"model_evaluation\"))\n",
    "\n",
    "            for file in all_files:\n",
    "                if file not in self.config.ALL_REQUIRED_FILES:\n",
    "                    Evaluation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\")\n",
    "                else:\n",
    "                    Evaluation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Evaluation status: {Evaluation_status}\"+\",\")\n",
    "                        f.write(f\"accuracyScore: {accuracyScore}\"+\",\")\n",
    "                        f.write(f\"recallScore: {recallScore}\"+\",\")\n",
    "                        f.write(f\"precisionScore: {precisionScore}\"+\",\")\n",
    "                        f.write(f\"f1Score: {f1Score}\"+\",\")\n",
    "\n",
    "            return Evaluation_status\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment.config.configuration import ConfigurationManager\n",
    "from Sentiment.components.model_trainer import ModelTrainer\n",
    "from Sentiment.components.model_evaluation import ModelEvaluation\n",
    "from Sentiment import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 12:51:31,588: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-05 12:51:31,594: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-01-05 12:51:31,597: INFO: common: yaml file: {yaml_file}]\n",
      "[2024-01-05 12:51:31,602: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-05 12:51:31,606: INFO: common: created directory at: artifacts]\n",
      "[2024-01-05 12:51:31,608: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-01-05 12:51:31,610: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2024-01-05 12:51:31,611: INFO: 1760456339: data loading Started]\n",
      "[2024-01-05 12:51:49,594: INFO: 1760456339: data loading Completed]\n",
      "[2024-01-05 12:51:49,595: INFO: 1760456339: data Lowercase convertion Started]\n",
      "[2024-01-05 12:51:49,844: INFO: model_trainer: Data converted to lowercase]\n",
      "[2024-01-05 12:51:49,844: INFO: 1760456339: data Lowercase convertion Completed]\n",
      "[2024-01-05 12:51:49,844: INFO: 1760456339: data Splitting Started]\n",
      "[2024-01-05 12:51:50,027: INFO: 1760456339: data Splitting Completed]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager()\n",
    "model_trainer_config = config.get_model_trainer_config()\n",
    "model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "model_evaluation_config = config.get_model_evaluation_config()\n",
    "model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n",
    "logger.info(f\"data loading Started\")\n",
    "sentiData = model_trainer_config.loadData()\n",
    "logger.info(f\"data loading Completed\")\n",
    "logger.info(f\"data Lowercase convertion Started\")\n",
    "sentiData = model_trainer_config.convertToLower(sentiData)\n",
    "logger.info(f\"data Lowercase convertion Completed\")\n",
    "logger.info(f\"data Splitting Started\")\n",
    "trainSentences, testSentences, trainLabels, testLabels = model_trainer_config.splitData(sentiData)\n",
    "logger.info(f\"data Splitting Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 13:00:20,625: INFO: 3100721655: Model Loading]\n",
      "[2024-01-05 13:00:21,105: INFO: 3100721655: Model Loaded]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Model Loading\")\n",
    "model = model_evaluation_config.loadModel()\n",
    "logger.info(f\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokens(plaintext):\n",
    "        text = []\n",
    "        # print(plaintext)\n",
    "        text.append(plaintext)\n",
    "        # print(text)\n",
    "        vocab_size = 3000 # choose based on statistics\n",
    "        oov_tok = ''\n",
    "        max_length = 200\n",
    "        # text = [\"This is good i have ever ate\"]\n",
    "        tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        sequences = tokenizer.texts_to_sequences(text)\n",
    "        # print(\"sequences\",sequences)\n",
    "        # pad the sequence\n",
    "        padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "        # print(padded)\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7405616]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = Tokens('worst food i have ever eat and not good food')\n",
    "model.predict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-05 12:54:17,505: INFO: 1520779367: Predicting Test Data]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20304\\1520779367.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicting Test Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_evaluation_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestSentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Predicting Test Data Completed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\src\\Sentiment\\components\\model_evaluation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, testSentences, model)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredictTest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestSentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         '''\n\u001b[0;32m     21\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0mSentence\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         '''\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestSentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Predicting Test Data\")\n",
    "predictions = model_evaluation_config.predictTest(model,testSentences)\n",
    "logger.info(f\"Predicting Test Data Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"EvaluationMetrics Started\")\n",
    "accuracyScore,recallScore,precisionScore,f1Score = model_evaluation_config.EvaluationMetrics(predictions,testLabels)\n",
    "logger.info(f\"EvaluationMetrics Completed\")\n",
    "model_evaluation_config.ModelEvaluationStatus(accuracyScore,recallScore,precisionScore,f1Score)\n",
    "logger.info(f\"EvaluationMetrics Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokens(plaintext):\n",
    "        text = []\n",
    "        # print(plaintext)\n",
    "        text.append(plaintext)\n",
    "        # print(text)\n",
    "        vocab_size = 3000 # choose based on statistics\n",
    "        oov_tok = ''\n",
    "        max_length = 200\n",
    "        # text = [\"This is good i have ever ate\"]\n",
    "        tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        sequences = tokenizer.texts_to_sequences(text)\n",
    "        # print(\"sequences\",sequences)\n",
    "        # pad the sequence\n",
    "        padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "        # print(padded)\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.1.2-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\akish.pothuri\\python\\sentimentanalysis\\sentimentapi\\cancer\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.1.2-cp38-cp38-win_amd64.whl (192.3 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    label = torch.tensor(self.labels[idx])\n",
    "\n",
    "    encoding = self.tokenizer(text, truncation=True, padding=\"max_length\",\n",
    "                              max_length=self.max_len)\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'],\n",
    "        'attention_mask': encoding['attention_mask'],\n",
    "        'labels': label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py:448\u001b[0m, in \u001b[0;36mAutoModelForSequenceClassification.from_pretrained\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 448\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akish.pothuri\\python\\SentimentAnalysis\\SentimentAPI\\cancer\\lib\\site-packages\\transformers\\file_utils.py:569\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    567\u001b[0m name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(BACKENDS_MAPPING[backend][\u001b[38;5;241m0\u001b[39m]() \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends):\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([BACKENDS_MAPPING[backend][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends]))\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'distilbert-base-uncased'\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
